import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
import chardet
from pykalman import KalmanFilter
import numpy as np

def apply_kalman_filter(rssi_values):
    """
    ì¹¼ë§Œ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ RSSI ê°’ì„ ë³´ì •í•˜ëŠ” í•¨ìˆ˜
    """
    kf = KalmanFilter(initial_state_mean=rssi_values[0], n_dim_obs=1)
    filtered_rssi, _ = kf.smooth(rssi_values)
    return filtered_rssi


# ğŸ“Œ ë°ì´í„° ë¡œë“œ
file_path = "./wifi_rssi_log.csv"

# ğŸ” íŒŒì¼ ì¸ì½”ë”© ê°ì§€ í›„ ë¡œë“œ
with open(file_path, "rb") as f:
    result = chardet.detect(f.read())
encoding_type = result['encoding']
print(f"ê°ì§€ëœ ì¸ì½”ë”©: {encoding_type}")

df = pd.read_csv(file_path, encoding=encoding_type)

# ğŸ“Œ MAC ì£¼ì†Œë¥¼ One-Hot Encoding
label_encoder_mac = LabelEncoder()
df["mac_encoded"] = label_encoder_mac.fit_transform(df["MAC"])
one_hot_encoder = OneHotEncoder(sparse=False)
mac_one_hot = one_hot_encoder.fit_transform(df["mac_encoded"].values.reshape(-1, 1))

# ğŸ“Œ RSSI ì •ê·œí™”
df["rssi_norm"] = (df["RSSI"] + 100) / 100
# ğŸ“Œ ì ìš© (ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì‹¤í–‰)
# df["rssi_norm"] = apply_kalman_filter(df["rssi_norm"])

# ğŸ“Œ Locationì„ Label Encoding (ë¬¸ìì—´ â†’ ìˆ«ì ë³€í™˜)
label_encoder_loc = LabelEncoder()
df["loc_encoded"] = label_encoder_loc.fit_transform(df["Location"])  # ì›ë˜ ê°’ì„ ìˆ«ìë¡œ ë³€í™˜

# ğŸ“Œ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
grouped = df.groupby(["Time", "loc_encoded"])

# ğŸ“Œ ë°ì´í„° ë³€í™˜
X_list, y_list = [], []
max_ap = 10  # í•œ ë²ˆì˜ ì¸¡ì •ì—ì„œ ìµœëŒ€ 10ê°œ AP ì‚¬ìš©

for (time, location), group in grouped:
    mac_one_hot_group = one_hot_encoder.transform(label_encoder_mac.transform(group["MAC"]).reshape(-1, 1))
    rssi_values = group["rssi_norm"].values

    # MAC ì£¼ì†Œ + RSSI ê²°í•©
    feature_vector = np.hstack([mac_one_hot_group, rssi_values.reshape(-1, 1)])

    # íŒ¨ë”© ì¶”ê°€ (AP ê°œìˆ˜ê°€ max_apë³´ë‹¤ ì ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ìš°ê¸°)
    if feature_vector.shape[0] < max_ap:
        pad_size = max_ap - feature_vector.shape[0]
        feature_vector = np.vstack([feature_vector, np.zeros((pad_size, feature_vector.shape[1]))])
    elif feature_vector.shape[0] > max_ap:
        feature_vector = feature_vector[:max_ap]  # ì´ˆê³¼í•˜ë©´ ì˜ë¼ë²„ë¦¼

    X_list.append(feature_vector)
    y_list.append(location)  # ì´ë¯¸ LabelEncoderë¡œ ë³€í™˜í•œ ê°’ ì‚¬ìš©

# ğŸ“Œ ìµœì¢… ë°ì´í„°ì…‹ ë³€í™˜
X = np.array(X_list)
y = np.array(y_list)

# ë°ì´í„° í¬ê¸° ì¶œë ¥
print(f"ì´ í–‰ ìˆ˜: {df.shape[0]}")
print(f"ì´ ë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜: {len(X)}")
print(f"ì…ë ¥ ë°ì´í„° í¬ê¸°: {X.shape}")  # (ìƒ˜í”Œ ìˆ˜, AP ê°œìˆ˜, MAC + RSSI)
print(f"ì¶œë ¥ ë°ì´í„° í¬ê¸°: {y.shape}")  # (ìƒ˜í”Œ ìˆ˜,)
print(f"ì‚¬ìš©ëœ Location ë¼ë²¨: {list(label_encoder_loc.classes_)}")  # ì›ë˜ ë¬¸ìì—´ Location ê°’

# ğŸ“Œ Train/Test Split
test_size = 0.2
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)


# ğŸ“Œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class WifiDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ğŸ“Œ ë°ì´í„° ë¡œë” ìƒì„±
train_dataset = WifiDataset(X_train, y_train)
test_dataset = WifiDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)


# ğŸ“Œ CNN ëª¨ë¸ ì •ì˜
class DeepWifiCNN(nn.Module):
    def __init__(self, num_ap, num_features, num_classes):
        super(DeepWifiCNN, self).__init__()
        
        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(64)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(128)
        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm1d(256)

        self.dropout = nn.Dropout(0.3)  # Dropout ì¶”ê°€
        self.fc1 = nn.Linear(256 * num_ap, 512)
        self.fc2 = nn.Linear(512, num_classes)

        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.permute(0, 2, 1)
        
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.relu(self.bn3(self.conv3(x)))
        
        x = x.view(x.size(0), -1)  # Flatten
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        
        return x


# ğŸ“Œ ëª¨ë¸ ì´ˆê¸°í™”
num_ap = X.shape[1]
num_features = X.shape[2]
num_classes = len(set(y))

model = DeepWifiCNN(num_ap, num_features, num_classes)
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label Smoothing ì ìš©
optimizer = optim.Adam(model.parameters(), lr=0.0005)

# ğŸ“Œ ëª¨ë¸ í•™ìŠµ
num_epochs = 2000
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # ğŸ”¥ ë§¤ Epoch í›„ ì •í™•ë„ ê³„ì‚°
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            total += y_batch.size(0)
            correct += (predicted == y_batch).sum().item()

    accuracy = 100 * correct / total
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%")


# ğŸ“Œ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_location(model, label_encoder_mac, one_hot_encoder, label_encoder_loc, mac_rssi_dict, num_ap, num_features):
    input_vector = np.zeros((num_ap, num_features))

    for i, (mac, rssi) in enumerate(mac_rssi_dict.items()):
        if i >= num_ap:
            break
        if mac in label_encoder_mac.classes_:
            mac_one_hot = one_hot_encoder.transform([[label_encoder_mac.transform([mac])[0]]])[0]
            rssi_norm = (rssi + 100) / 100
            input_vector[i] = np.hstack([mac_one_hot, rssi_norm])

    input_tensor = torch.tensor(input_vector.reshape(1, num_ap, num_features), dtype=torch.float32)

    model.eval()
    with torch.no_grad():
        output = model(input_tensor)
        _, predicted_index = torch.max(output, 1)

    predicted_location = label_encoder_loc.inverse_transform([predicted_index.item()])[0]

    return predicted_location


# ğŸ“Œ ì˜ˆì¸¡ ì˜ˆì‹œ
test_input = {
    "26:3f:0b:e2:66:14": -40,
    "26:3f:0b:e2:64:73": -75,
    "26:3f:1b:59:2e:56": -87,
    "26:3f:0b:e2:66:b0": -65,
    "26:3f:0b:e2:65:61": -77,
    "26:3f:1b:e2:66:3f": -77,
    "ee:55:b8:6e:4b:9c": -87,
    "26:3f:1b:59:2e:20": -83,
    "26:3f:0b:e2:67:13": -66,
    "26:3f:0b:e2:66:3f": -75,
    "26:3f:1b:e2:66:b0": -77,
}

predicted_location = predict_location(model, label_encoder_mac, one_hot_encoder, label_encoder_loc, test_input, num_ap, num_features)
print(f"ì˜ˆì¸¡ëœ ìœ„ì¹˜: {predicted_location}")