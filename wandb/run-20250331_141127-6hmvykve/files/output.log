[34m[1mwandb[0m: [33mWARNING[0m Config item 'embedding_dim' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'transformer_heads' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'transformer_layers' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'batch_size' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'scheduler' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'early_stopping' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'epochs' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'data_path' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: Ctrl + C detected. Stopping sweep.
Traceback (most recent call last):
  File "/Users/gichanpark/Desktop/castone_design/finger_printing/train_CNNTransformer_sweep.py", line 183, in train_model
    optimizer.step()
  File "/opt/homebrew/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/opt/homebrew/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/opt/homebrew/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/homebrew/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/opt/homebrew/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/opt/homebrew/anaconda3/envs/capstone/lib/python3.8/site-packages/torch/optim/adam.py", line 432, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
Exception
